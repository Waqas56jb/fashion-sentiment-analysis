{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eae4a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fast laptop\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 541.6 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 541.6 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 516.5 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 516.5 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 516.5 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 445.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 445.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 445.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 445.6 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 445.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 351.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 351.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 351.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 351.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 320.9 kB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f5fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\FAST\n",
      "[nltk_data]     LAPTOP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\FAST\n",
      "[nltk_data]     LAPTOP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review sentiment\n",
      "0        absolutely wonderful silky sexy comfortable  Positive\n",
      "1  love dress sooo pretty happened find store im ...  Positive\n",
      "2  high hopes dress really wanted work initially ...   Neutral\n",
      "3  love love love jumpsuit fun flirty fabulous ev...  Positive\n",
      "4  shirt flattering due adjustable front tie perf...  Positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"../dataset/data_amazon.xlsx - Sheet1.csv\")  # Replace with your actual filename\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['Review', 'Cons_rating']]\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(subset=['Review', 'Cons_rating'], inplace=True)\n",
    "\n",
    "# Function to clean the review text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "    tokens = word_tokenize(text)  # tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Generate sentiment labels\n",
    "def label_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'Negative'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "df['sentiment'] = df['Cons_rating'].apply(label_sentiment)\n",
    "\n",
    "# Final cleaned DataFrame\n",
    "cleaned_df = df[['cleaned_review', 'sentiment']]\n",
    "\n",
    "# Save cleaned data if needed\n",
    "cleaned_df.to_csv(\"cleaned_fashion_reviews.csv\", index=False)\n",
    "\n",
    "# Display few rows\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7d098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "LogisticRegression Accuracy: 0.8168\n",
      "Training NaiveBayes...\n",
      "NaiveBayes Accuracy: 0.7928\n",
      "Training SVM...\n",
      "SVM Accuracy: 0.8144\n",
      "Training RandomForest...\n",
      "RandomForest Accuracy: 0.7989\n",
      "Training KNN...\n",
      "KNN Accuracy: 0.7487\n",
      "\n",
      "âœ… Best model: LogisticRegression (Accuracy: 0.8168)\n",
      "ðŸ“¦ Model saved as: LogisticRegression_sentiment_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "df=cleaned_df\n",
    "\n",
    "# Features and Labels\n",
    "X = df['cleaned_review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define 5 pipelines with model and TF-IDF\n",
    "pipelines = {\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ]),\n",
    "    'NaiveBayes': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LinearSVC())\n",
    "    ]),\n",
    "    'RandomForest': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'KNN': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Define hyperparameters to tune for each model\n",
    "param_grid = {\n",
    "    'LogisticRegression': {\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'clf__alpha': [0.5, 1.0, 1.5]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__max_depth': [None, 10]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'clf__n_neighbors': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# GridSearch over all models\n",
    "best_score = 0\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    grid = GridSearchCV(pipe, param_grid[name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n",
    "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_score:\n",
    "        best_score = acc\n",
    "        best_model = grid.best_estimator_\n",
    "        best_model_name = name\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, f\"{best_model_name}_sentiment_model.pkl\")\n",
    "print(f\"\\nâœ… Best model: {best_model_name} (Accuracy: {best_score:.4f})\")\n",
    "print(f\"ðŸ“¦ Model saved as: {best_model_name}_sentiment_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
